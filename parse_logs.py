import glob
import re
import pandas as pd
import math
import statistics
import os

EVAL_ROOT = "final_benchmark_results"

def parse_agent_log(log_path):
    """Extracts the dataset name and the final metric from a single log file."""
    with open(log_path, 'r', encoding='utf-8', errors='ignore') as f:
        content = f.read()

    # 1. Find Dataset Name (often logged by the agent or derived from the run path)
    # We'll use the path structure if the agent doesn't log it explicitly.
    # The structure is EVAL_ROOT/dataset_name/seed_X/...
    dataset_match = re.search(r"{}[/]([^/]+)[/]".format(EVAL_ROOT), log_path)
    dataset = dataset_match.group(1) if dataset_match else "Unknown"
    
    # 2. Find Metrics
    metrics_strs = re.findall(r"\{'eval_.*?'\}", content)
    
    score = None
    metric_type = "N/A"
    
    if metrics_strs:
        # Take the last reported metric (final epoch)
        last_metric = eval(metrics_strs[-1])
        
        if 'eval_accuracy' in last_metric:
            score = float(last_metric['eval_accuracy'])
            metric_type = "Accuracy"
        elif 'eval_rmse' in last_metric:
            # Note: Lower is better for RMSE/Loss
            score = float(last_metric['eval_rmse'])
            metric_type = "RMSE"
        elif 'eval_loss' in last_metric:
            # Note: Lower is better for RMSE/Loss
            score = float(last_metric['eval_loss'])
            metric_type = "Log Loss"
            
    return dataset, metric_type, score


def calculate_statistics():
    """Reads logs from the structured results directory and calculates Mean Â± SE."""
    
    # Finds all logs generated by the fixed bash script
    log_files = glob.glob(f"{EVAL_ROOT}/**/agent_run_seed_*.log", recursive=True)
    
    results = {}
    print(f"ğŸ“‚ Found {len(log_files)} archived logs. Calculating statistics...")

    for log in log_files:
        dataset, metric_type, score = parse_agent_log(log)
        
        if dataset != "Unknown" and score is not None:
            if dataset not in results:
                results[dataset] = {'metric': metric_type, 'scores': []}
            
            # Group scores by dataset
            results[dataset]['scores'].append(score)

    final_data = []
    
    for dataset, data in results.items():
        scores = data['scores']
        n = len(scores)
        
        if n > 0:
            mean_score = statistics.mean(scores)
            
            # Calculate Standard Error (StdDev / sqrt(N))
            std_error = 0.0
            if n > 1:
                stdev = statistics.stdev(scores)
                std_error = stdev / math.sqrt(n)
            
            # Format: "0.8540 Â± 0.0021"
            formatted_result = f"{mean_score:.4f} Â± {std_error:.4f}"
            
            final_data.append({
                "Dataset": dataset,
                "Metric": data['metric'],
                "Runs (Seeds)": n,
                "Validation Score (Mean Â± SE)": formatted_result
            })

    # Print Markdown Table
    if final_data:
        df = pd.DataFrame(final_data)
        
        print("\n\nâœ… Copy this table into your README.md:\n")
        print("| Dataset | Metric | Runs | Score (Mean Â± SE) |")
        print("| :--- | :--- | :--- | :--- |")
        for _, row in df.iterrows():
            print(f"| {row['Dataset']} | {row['Metric']} | {row['Runs (Seeds)']} | {row['Validation Score (Mean Â± SE)']} |")
    else:
        print("âŒ No metrics found. Run the fixed benchmark script first!")

if __name__ == "__main__":
    calculate_statistics()